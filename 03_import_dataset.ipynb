{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "# Tfidf transformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#https://www.nltk.org/howto/stem.html\n",
    "from nltk.stem.porter import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('src/train.csv')\n",
    "dataset = dataset.drop(columns=['id', 'location'])\n",
    "dataset = dataset.dropna()\n",
    "dataset['keyword'] = [ x.replace('%20', ' ') for x in dataset['keyword']]\n",
    "len(dataset['keyword'].unique())\n",
    "\n",
    "# https://medium.com/@yashj302/stopwords-nlp-python-4aa57dc492af\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def cleanStopWorld(text):\n",
    "    cleanedtext = []\n",
    "    for item in nlp(text):\n",
    "        if not item.is_stop:\n",
    "            cleanedtext.append(item.text)\n",
    "    return ' '.join(cleanedtext)\n",
    "\n",
    "def eraseSingleChar(words):\n",
    "    new_text = \"\"\n",
    "    for w in list(words.split(\" \")):\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "# convert exemple \"Player\", \"Playing\" to \"play\"\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemConverter(sentence):\n",
    "    new_text = \"\"\n",
    "    for w in list(sentence.split(\" \")):\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "# from numbers to string \n",
    "def convert_num_to_words(utterance):\n",
    "      utterance = ' '.join([num2words.num2words(i) if i.isdigit() else i for i in utterance.split()])\n",
    "      return utterance\n",
    "\n",
    "for column in ['keyword', 'text']:\n",
    "    dataset[column] = [ np.char.lower(x) for x in dataset[column]]\n",
    "    dataset[column] = [ str(x) for x in dataset[column] ] # str check for \"numbers\"\n",
    "    dataset[column] = [ x.replace(\"\\\\/\", \"/\").encode().decode('utf-8') for x in dataset[column] ] # magic line for error byte string\n",
    "    dataset[column] = [ re.sub(r'http\\S+', '', x) for x in dataset[column] ] # delete http \n",
    "    dataset[column] = [ re.sub('[^A-Za-z0-9 ]+', '', x) for x in dataset[column] ] # delete @#$â‚¬...\n",
    "    #\n",
    "    dataset[column] = [ cleanStopWorld(x) for x in dataset[column] ] # erase stop world \n",
    "    #\n",
    "    dataset[column] = [ eraseSingleChar(x) for x in dataset[column] ] # erase isolate 2 chars\n",
    "    #\n",
    "    dataset[column] = [ stemConverter(x) for x in dataset[column] ] # convert to stem words\n",
    "\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "dataset['text'] = dataset['keyword'].astype(str) + dataset['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1000, oov_token=\"out_of_vocab\") # instanciate the tokenizer\n",
    "tokenizer.fit_on_texts(dataset.text)\n",
    "dataset[\"text_encoded\"] = tokenizer.texts_to_sequences(dataset.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>ablaz  bbcmtd wholesal market ablaz</td>\n",
       "      <td>1</td>\n",
       "      <td>[213, 1, 1, 310, 213]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>ablaz  tri bring heavi metal rt</td>\n",
       "      <td>0</td>\n",
       "      <td>[213, 220, 453, 697, 962, 90]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>ablaz  africanbaz break newsnigeria flag set...</td>\n",
       "      <td>1</td>\n",
       "      <td>[213, 1, 270, 1, 527, 221, 213, 910]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>ablaz  cri set ablaz</td>\n",
       "      <td>0</td>\n",
       "      <td>[213, 624, 221, 213]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>ablaz  plu look sky night ablaz</td>\n",
       "      <td>0</td>\n",
       "      <td>[213, 1, 48, 724, 285, 213]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    keyword                                               text  target  \\\n",
       "31    ablaz                ablaz  bbcmtd wholesal market ablaz       1   \n",
       "32    ablaz                    ablaz  tri bring heavi metal rt       0   \n",
       "33    ablaz    ablaz  africanbaz break newsnigeria flag set...       1   \n",
       "34    ablaz                               ablaz  cri set ablaz       0   \n",
       "35    ablaz                    ablaz  plu look sky night ablaz       0   \n",
       "\n",
       "                            text_encoded  \n",
       "31                 [213, 1, 1, 310, 213]  \n",
       "32         [213, 220, 453, 697, 962, 90]  \n",
       "33  [213, 1, 270, 1, 527, 221, 213, 910]  \n",
       "34                  [213, 624, 221, 213]  \n",
       "35           [213, 1, 48, 724, 285, 213]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(r'src/dataset_encoded.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
