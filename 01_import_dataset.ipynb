{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "#https://www.nltk.org/howto/stem.html\n",
    "from nltk.stem.porter import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('src/train.csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           0.000000\n",
      "keyword      0.801261\n",
      "location    33.272035\n",
      "text         0.000000\n",
      "target       0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pourcentage_valeur_manquante = 100*dataset.isnull().sum()/dataset.shape[0]\n",
    "print(pourcentage_valeur_manquante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=['id', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset['keyword'] = [ x.replace('%20', ' ') for x in dataset['keyword']]\n",
    "len(dataset['keyword'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7552"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full', 'nevertheless', 'are', 'someone', 'nobody', 'be', 'go', 'then', 'same', 'or', 'ten', 'each', 'your', 'something', 'everything', \"'re\", 'yet', 'anywhere', 'than', 'off', 'amongst', 'again', 'mostly', 'really', 'less', '’d', \"'d\", 'call', 'becoming', 'due', 'former', 'could', 'he', 'thereby', 'seemed', 'fifty', 'formerly', 'might', 'whole', 'six', '‘m', 'neither', 'part', 'various', 'from', 'moreover', 'via', 'still', 'already', 'there', 'serious', 'because', 'is', 'what', 'many', 'several', 'so', 'will', \"'ll\", 'to', 'few', 'mine', 'therein', 'n’t', 'somewhere', 'that', 'everyone', 'say', 'hers', 'herein', 'whose', 'please', 'never', 'made', 'toward', 'though', '’ve', 'herself', 'about', 'not', 'however', 'at', '’m', 'towards', 'when', 'perhaps', 'can', 'below', 'get', \"'s\", 'fifteen', 'how', 'hence', 'whereas', 'own', 'along', 'on', 'her', 'against', 'eleven', 'our', 'among', 'over', 'done', 'put', 'empty', 'but', 'upon', 'give', 'somehow', 'whether', 'beyond', 'ca', 'alone', 'had', 'nowhere', 'must', 'too', 'whereby', 'since', 'every', 'behind', 'sometime', 'third', 'have', 'who', 'anyone', 'in', 'except', 'least', 'ours', 'would', 'its', 'while', 'wherein', 'last', 'whereupon', 'all', 'even', \"'m\", 'of', 'by', 'thereupon', 'anything', 'keep', 'whither', 'she', 'thru', 'may', 'hereupon', 'most', 'them', '‘d', 'almost', 'either', 'been', 'after', 'enough', 'throughout', 'being', 'whoever', 'some', 'therefore', 'very', 'two', 'seeming', 'well', 'through', 'these', 'any', 'become', 'above', 'whence', 'none', 'five', 'used', 'amount', 'before', 'thence', '‘ve', 'the', 'often', 'latter', 'twelve', 'him', 'besides', 'for', '’re', 'which', 'sixty', 'until', 'meanwhile', 'me', 'together', 'name', 'afterwards', 'as', 'otherwise', 'themselves', 'much', 'ever', 'why', 'it', 'such', 'i', 'we', 'seem', 'no', 'unless', 'became', 'else', 'yourselves', 'sometimes', 'my', 'back', 'becomes', '‘s', 'itself', 'cannot', 'nor', 'if', 'between', 'four', 'more', 'latterly', 'nothing', 'do', '’s', 'n‘t', 'now', 'rather', 'himself', 'once', 'one', '‘re', 'here', 'hereafter', 'did', 'others', 'further', 'another', 'whenever', 'has', 'us', 'beforehand', 'am', 'top', 'noone', 'regarding', 'anyway', 'their', 'yourself', 'nine', 'show', 'whom', 'whereafter', 'just', 'a', 'always', 'anyhow', 'and', 'should', 'everywhere', 'elsewhere', 'move', 'bottom', 'with', 'onto', 'forty', 'namely', 'they', 'where', 'twenty', 'other', 'three', 'also', 'around', 'indeed', 'was', 'yours', 'see', 'were', 'only', 'into', 'myself', 'without', \"'ve\", 'per', 'out', 'down', 'although', 'whatever', 'this', 'first', 'ourselves', 're', 'make', 'within', 'wherever', 'an', 'hundred', 'front', 'those', 'thus', 'thereafter', '‘ll', 'his', 'quite', 'under', 'side', 'both', 'using', 'does', 'up', 'hereby', '’ll', 'doing', 'next', 'seems', 'during', 'eight', 'across', 'take', 'beside', 'you', \"n't\"}\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@yashj302/stopwords-nlp-python-4aa57dc492af\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "print(stopwords)\n",
    "\n",
    "def cleanStopWorld(text):\n",
    "    cleanedtext = []\n",
    "    for item in nlp(text):\n",
    "        if not item.is_stop:\n",
    "            cleanedtext.append(item.text)\n",
    "    return ' '.join(cleanedtext)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eraseSingleChar(words):\n",
    "    new_text = \"\"\n",
    "    for w in list(words.split(\" \")):\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert exemple \"Player\", \"Playing\" to \"play\"\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemConverter(sentence):\n",
    "    new_text = \"\"\n",
    "    for w in list(sentence.split(\" \")):\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numbers to string \n",
    "def convert_num_to_words(utterance):\n",
    "      utterance = ' '.join([num2words.num2words(i) if i.isdigit() else i for i in utterance.split()])\n",
    "      return utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['keyword', 'text']:\n",
    "    dataset[column] = [ np.char.lower(x) for x in dataset[column]]\n",
    "    dataset[column] = [ str(x) for x in dataset[column] ] # str check for \"numbers\"\n",
    "    dataset[column] = [ x.replace(\"\\\\/\", \"/\").encode().decode('utf-8') for x in dataset[column] ] # magic line for error byte string\n",
    "    dataset[column] = [ re.sub(r'http\\S+', '', x) for x in dataset[column] ] # delete http \n",
    "    dataset[column] = [ re.sub('[^A-Za-z0-9 ]+', '', x) for x in dataset[column] ] # delete @#$€...\n",
    "    #\n",
    "    dataset[column] = [ cleanStopWorld(x) for x in dataset[column] ] # erase stop world \n",
    "    #\n",
    "    dataset[column] = [ eraseSingleChar(x) for x in dataset[column] ] # erase isolate 2 chars\n",
    "    #\n",
    "    dataset[column] = [ stemConverter(x) for x in dataset[column] ] # convert to stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>bbcmtd wholesal market ablaz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>tri bring heavi metal rt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>africanbaz break newsnigeria flag set ablaz aba</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>cri set ablaz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ablaz</td>\n",
       "      <td>plu look sky night ablaz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>wreck</td>\n",
       "      <td>jtruff23 cameronhack wreck</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>wreck</td>\n",
       "      <td>day work ve pretti wreck hahaha shoutout famili</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>wreck</td>\n",
       "      <td>fx forex trade cramer iger word wreck disney...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>wreck</td>\n",
       "      <td>enginesh great atmospher british lion gig to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582</th>\n",
       "      <td>wreck</td>\n",
       "      <td>cramer iger word wreck disney stock cnbc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword                                               text  target\n",
       "31      ablaz                       bbcmtd wholesal market ablaz       1\n",
       "32      ablaz                           tri bring heavi metal rt       0\n",
       "33      ablaz    africanbaz break newsnigeria flag set ablaz aba       1\n",
       "34      ablaz                                      cri set ablaz       0\n",
       "35      ablaz                           plu look sky night ablaz       0\n",
       "...       ...                                                ...     ...\n",
       "7578    wreck                         jtruff23 cameronhack wreck       0\n",
       "7579    wreck    day work ve pretti wreck hahaha shoutout famili       0\n",
       "7580    wreck    fx forex trade cramer iger word wreck disney...       0\n",
       "7581    wreck    enginesh great atmospher british lion gig to...       0\n",
       "7582    wreck           cramer iger word wreck disney stock cnbc       0\n",
       "\n",
       "[7552 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
